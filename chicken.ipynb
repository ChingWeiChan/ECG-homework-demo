{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxixVhwcL73z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Install the requirement and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiKanGlwbCuN",
    "outputId": "0041812b-776b-4d5a-bf38-752efa37f529",
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "!mkdir dataset\n",
    "os.chdir(\"./dataset\")\n",
    "!curl -L \"https://universe.roboflow.com/ds/qsAqxl1yWz?key=cc6BA6xJi0\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
    "!pip install pycocotools\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ba-6X_qQMG6d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d1PiIviDc1p2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 5e-4\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 30\n",
    "NUM_WORKERS = 4\n",
    "CHECKPOINT_FILE = \"Best.pth.tar\"\n",
    "PIN_MEMORY = True\n",
    "SAVE_MODEL = False\n",
    "LOAD_MODEL = False\n",
    "TRAIN_DIR = './dataset/train'\n",
    "VALID_DIR = './dataset/valid'\n",
    "TEST_DIR = './dataset/test'\n",
    "IMAGE_SIZE = [1152,648]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McR1BFjKMljL",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nCRdKYYIdG44",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class CCDataset(Dataset):\n",
    "  def __init__(self, mode = 'train', augmentation=None):\n",
    "    if mode == 'train':\n",
    "      self.dataset_path = TRAIN_DIR\n",
    "      ann_path = os.path.join(TRAIN_DIR, '_annotations.coco.json')\n",
    "    if mode == 'valid':\n",
    "      self.dataset_path = VALID_DIR\n",
    "      ann_path = os.path.join(VALID_DIR, '_annotations.coco.json')\n",
    "    if mode == 'test':\n",
    "      self.dataset_path = TEST_DIR\n",
    "      ann_path = os.path.join(TEST_DIR, '_annotations.coco.json')\n",
    "    \n",
    "    self.coco = COCO(ann_path)\n",
    "    self.cat_ids = self.coco.getCatIds()\n",
    "    self.augmentation=augmentation\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.coco.imgs)\n",
    "  \n",
    "  def get_masks(self, index):\n",
    "      ann_ids = self.coco.getAnnIds([index])\n",
    "      anns = self.coco.loadAnns(ann_ids)\n",
    "      masks=[]\n",
    "\n",
    "      for ann in anns:\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "\n",
    "      return masks\n",
    "\n",
    "  def get_boxes(self, masks):\n",
    "      num_objs = len(masks)\n",
    "      boxes = []\n",
    "\n",
    "      for i in range(num_objs):\n",
    "          x,y,w,h = cv2.boundingRect(masks[i])\n",
    "          boxes.append([x, y, x+w, y+h])\n",
    "\n",
    "      return np.array(boxes)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "      # Load image\n",
    "      img_info = self.coco.loadImgs([index])[0]\n",
    "      image = cv2.imread(os.path.join(self.dataset_path,\n",
    "                                    img_info['file_name']))\n",
    "      masks = self.get_masks(index)\n",
    "\n",
    "      if self.augmentation:\n",
    "        augmented = self.augmentation(image=image, masks=masks)\n",
    "        image, masks = augmented['image'], augmented['masks']\n",
    "\n",
    "      image = image.transpose(2,0,1) / 255.\n",
    "\n",
    "      # Load masks\n",
    "      masks = np.array(masks)\n",
    "      boxes = self.get_boxes(masks)\n",
    "\n",
    "      # Create target dict\n",
    "      num_objs = len(masks)\n",
    "      boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "      labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "      masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "      image = torch.as_tensor(image, dtype=torch.float32)\n",
    "      data = {}\n",
    "      data[\"boxes\"] =  boxes\n",
    "      data[\"labels\"] = labels\n",
    "      data[\"masks\"] = masks\n",
    "\n",
    "      return image, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ng_1Du3EdHkK",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# To bypass shape errors\n",
    "# Images contain different number of instances\n",
    "def collate_fn(batch):\n",
    "  images = list()\n",
    "  targets = list()\n",
    "  for b in batch:\n",
    "        images.append(b[0])\n",
    "        targets.append(b[1])\n",
    "  images = torch.stack(images, dim=0)\n",
    "  return images, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JutigjxIM4Xx",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9gpwL8htdMP3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(\n",
    "        contrast_limit=0.2, brightness_limit=0.3, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.ImageCompression(p=0.8),\n",
    "        A.RandomGamma(p=0.8),\n",
    "        A.Blur(p=0.8),\n",
    "        A.Equalize(mode='cv',p=0.8)\n",
    "    ], p=1.0),\n",
    "    A.OneOf([\n",
    "        A.ImageCompression(p=0.8),\n",
    "        A.RandomGamma(p=0.8),\n",
    "        A.Blur(p=0.8),\n",
    "        A.Equalize(mode='cv',p=0.8),\n",
    "    ], p=1.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fji0WQ_TNGP_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRFSxPUZNInB",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F_B03cXYdOD5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EdqDD8sadQ0X",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(\n",
    "            in_features, num_classes=1+1)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Drro1fXBdUHm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"mask_rcnn.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dFUXowb6dXJ2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    #optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZwKUprI6NXrR",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(loader, model, optimizer, device):\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(loop):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"Total loss: {losses.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hCh0m3dTNacG",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "best_vloss = np.inf\n",
    "def validate(loader, model, optimizer, device, epoch):\n",
    "    global best_vloss\n",
    "    loop = tqdm(loader)\n",
    "    running_vloss = 0\n",
    "    for batch_idx, (images, targets) in enumerate(loop):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "          loss_dict = model(images, targets)\n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        running_vloss += losses\n",
    "        \n",
    "    avg_vloss = running_vloss / (batch_idx + 1)\n",
    "    \n",
    "    print(f\"Avg Valid Loss: {avg_vloss}\")\n",
    "    if avg_vloss < best_vloss:\n",
    "      best_vloss = avg_vloss\n",
    "      if SAVE_MODEL:\n",
    "            print(\"Model improved, saving...\")\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            save_checkpoint(checkpoint, filename=f\"1152KaggleBest_second_{epoch}.pth.tar\")\n",
    "    print('\\n')\n",
    "    return avg_vloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2GfEEXmNP7P",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-S8mFP7daEu",
    "outputId": "14e6c5a1-2918-46a0-8505-6b9a5dbb2f89",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CCDataset(mode='train', augmentation=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=2,\n",
    "                              pin_memory=PIN_MEMORY,\n",
    "                              collate_fn=collate_fn)\n",
    "\n",
    "valid_dataset = CCDataset(mode='valid')\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              pin_memory=PIN_MEMORY,\n",
    "                              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfIntiViN8vK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c6dcec9d20b24a05a7af407b4b08cef8",
      "707bc834692145cfb20fef5709ec9ff0",
      "363777814d10441f8f8c17da414642ae",
      "4048ab22f90245d280ea15c8c52baf6b",
      "32be7fb0cec14302b7b70e4bb4474aea",
      "c59ed527ae4640fda6dd63fee001ebd2",
      "763962acd39e48a19cd885fb30c30abb",
      "2c47ca7cac88405b81bf4caaaad63a50",
      "e3c677edd6c845e69a23f1ffeb9e65bc",
      "7b77dcf80a724b23a54d46e3ec0bda58",
      "bdaf9d65105e4027a5b029b4e2daa49a"
     ]
    },
    "id": "DS0Mi81rdgsY",
    "outputId": "441e086b-c1f9-4513-824d-a70554f29298",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
    "                                  lr=LEARNING_RATE,\n",
    "                                  weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "if LOAD_MODEL and CHECKPOINT_FILE in os.listdir():\n",
    "        print(\"Loading checkpoint\")\n",
    "        load_checkpoint(torch.load(CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "                print(f\"Epoch: {epoch}\")\n",
    "                train_one_epoch(train_loader, model, optimizer, DEVICE)\n",
    "                vloss= validate(valid_loader, model, optimizer, DEVICE, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaSity_1Nl66",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD2aWzsRdkfZ",
    "outputId": "616780ca-9471-45c7-b3bf-2ac7f094ee92",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1VZdpRVDPCjh3Ro9KbJtb9CN2lakHnUy_\n",
      "To: /content/test_video.mp4\n",
      "\r",
      "  0% 0.00/7.15M [00:00<?, ?B/s]\r",
      " 66% 4.72M/7.15M [00:00<00:00, 38.7MB/s]\r",
      "100% 7.15M/7.15M [00:00<00:00, 54.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1VZdpRVDPCjh3Ro9KbJtb9CN2lakHnUy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QQvtBNv-dp22",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def predict_single_frame(frame):\n",
    "    images = cv2.resize(frame, IMAGE_SIZE, cv2.INTER_LINEAR)/255\n",
    "    images = torch.as_tensor(images, dtype=torch.float32).unsqueeze(0)\n",
    "    images = images.swapaxes(1, 3).swapaxes(2, 3)\n",
    "    images = list(image.to(DEVICE) for image in images)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      pred = model(images)\n",
    "    \n",
    "    im = images[0].swapaxes(0, 2).swapaxes(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "    im2 = np.zeros_like(im).astype(np.float32)\n",
    "    for i in range(len(pred[0]['masks'])):\n",
    "        msk=pred[0]['masks'][i,0].detach().cpu().numpy()\n",
    "        scr=pred[0]['scores'][i].detach().cpu().numpy()\n",
    "        box=pred[0]['boxes'][i].detach().cpu().numpy()\n",
    "        \n",
    "        if scr>0.9 :\n",
    "            cv2.rectangle(im, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0,0,1), 2)\n",
    "            cv2.putText(im, \"{0:.2f}%\".format(scr*100), (int(box[0]+5), int(box[1])+15), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5, (0,0,1), 2, cv2.LINE_AA)\n",
    "            im2[:,:,0][msk>0.87] = np.random.uniform(0,1)\n",
    "            im2[:, :, 1][msk > 0.87] = np.random.uniform(0,1)\n",
    "            im2[:, :, 2][msk > 0.87] = np.random.uniform(0,1)\n",
    "\n",
    "    return (cv2.addWeighted(im, 0.8, im2, 0.2,0)*255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OF1bxwF-ds1n",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./test_video.mp4')\n",
    "model.train(False)\n",
    "\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "images = []   \n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        result_frame = predict_single_frame(frame)\n",
    "        images.append(result_frame)\n",
    "    else: \n",
    "        break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bahnPTC2dwPG",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "imageio.mimsave('./result.gif', images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OWLRmilKoaa",
    "outputId": "b00730cc-8843-428a-84f3-eef534e4253c",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "imageio.mimsave('/content/drive/MyDrive/result.gif', images)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "VxixVhwcL73z",
    "Ba-6X_qQMG6d",
    "McR1BFjKMljL",
    "fji0WQ_TNGP_",
    "lfIntiViN8vK",
    "AaSity_1Nl66"
   ],
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
